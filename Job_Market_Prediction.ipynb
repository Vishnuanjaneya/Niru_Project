{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data Preprocessing"
      ],
      "metadata": {
        "id": "eSkSl4AMqU4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def load_data(file_path):\n",
        "    \"\"\"Load the dataset from the specified CSV file.\"\"\"\n",
        "    df = pd.read_csv(file_path, sep=\"\\t\", engine='python')\n",
        "    return df\n",
        "\n",
        "def encode_categorical_variables(df):\n",
        "    \"\"\"Convert categorical variables into numerical format using one-hot encoding.\"\"\"\n",
        "    df = pd.get_dummies(df, columns=['qualification', 'skills'], drop_first=True)\n",
        "    return df\n",
        "\n",
        "def preprocess_data(file_path):\n",
        "    \"\"\"Preprocess the dataset.\"\"\"\n",
        "    df = load_data(file_path)\n",
        "\n",
        "    # Drop the 'name' column as it is not needed for model training\n",
        "    if 'name' in df.columns:\n",
        "        df.drop(columns=['name'], inplace=True)\n",
        "\n",
        "    # Fill missing values for numerical columns\n",
        "    for column in df.select_dtypes(include=['int64', 'float64']).columns:\n",
        "        df[column] = df[column].fillna(df[column].mean())\n",
        "\n",
        "    # Fill missing values for categorical columns\n",
        "    for column in df.select_dtypes(include=['object']).columns:\n",
        "        df[column] = df[column].fillna('Unknown')\n",
        "\n",
        "    # Convert categorical variables into numerical format\n",
        "    df = encode_categorical_variables(df)\n",
        "\n",
        "    # Ensure all numeric data types are floats\n",
        "    for column in df.select_dtypes(include=['int64', 'bool']).columns:\n",
        "        df[column] = df[column].astype(float)  # Convert int and bool columns to float\n",
        "\n",
        "    # Save the preprocessed DataFrame to a CSV file\n",
        "    output_path = '/content/preprocessed_candidates.csv'  # Update with Colab path\n",
        "    df.to_csv(output_path, index=False)\n",
        "    print(f\"Preprocessed data saved to {output_path}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Specify the dataset path in Colab\n",
        "DATASET_PATH = '/content/candidates.csv'  # Update with your path\n",
        "df = preprocess_data(DATASET_PATH)\n",
        "print(\"Preprocessed DataFrame:\")\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "-R5lleIqqSTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Model Training"
      ],
      "metadata": {
        "id": "0jxLR1k2rNm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def elastic_net(X, y, alpha=1.0, l1_ratio=0.5, num_iterations=1000, learning_rate=0.001):\n",
        "    \"\"\"Train the Elastic Net model.\"\"\"\n",
        "    num_samples, num_features = X.shape\n",
        "    weights = np.zeros(num_features)\n",
        "    bias = 0\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        # Calculate model predictions\n",
        "        model_predictions = np.dot(X, weights) + bias\n",
        "\n",
        "        # Calculate gradients\n",
        "        dw = (1 / num_samples) * np.dot(X.T, (model_predictions - y)) + alpha * (\n",
        "                    l1_ratio * np.sign(weights) + (1 - l1_ratio) * weights)\n",
        "        db = (1 / num_samples) * np.sum(model_predictions - y)\n",
        "\n",
        "        # Update weights and bias\n",
        "        weights -= learning_rate * dw\n",
        "        bias -= learning_rate * db\n",
        "\n",
        "        # Print debugging information every 100 iterations\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Iteration {i}: Weights: {weights}, Bias: {bias}, dw: {dw}, db: {db}\")\n",
        "\n",
        "    return weights, bias\n",
        "\n",
        "def main():\n",
        "    # Load preprocessed dataset\n",
        "    DATASET_PATH = '/content/preprocessed_candidates.csv'  # Update with Colab path\n",
        "    df = pd.read_csv(DATASET_PATH)\n",
        "\n",
        "    # Separate features and target variable\n",
        "    target_column = 'target'\n",
        "    X = df.drop(columns=[target_column])\n",
        "    y = df[target_column].values\n",
        "\n",
        "    # Print shapes and types for debugging\n",
        "    print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
        "    print(f\"X data types:\\n{X.dtypes}\")\n",
        "\n",
        "    # Train the Elastic Net model\n",
        "    weights, bias = elastic_net(X.values, y)\n",
        "\n",
        "    # Save weights, bias, and actual target values\n",
        "    np.save('/content/weights.npy', weights)\n",
        "    np.save('/content/bias.npy', bias)\n",
        "    np.save('/content/y_true.npy', y)\n",
        "\n",
        "    # Print weights and bias\n",
        "    print(\"Weights:\", weights)\n",
        "    print(\"Bias:\", bias)\n",
        "\n",
        "# Run the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "48podHz8rYAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Train and Save"
      ],
      "metadata": {
        "id": "Gr_Ib1xXrlM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def elastic_net(X, y, alpha=1.0, l1_ratio=0.5, num_iterations=1000, learning_rate=0.001):\n",
        "    \"\"\"Train the Elastic Net model.\"\"\"\n",
        "    num_samples, num_features = X.shape\n",
        "    weights = np.zeros(num_features)\n",
        "    bias = 0\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        # Calculate model predictions\n",
        "        model_predictions = np.dot(X, weights) + bias\n",
        "\n",
        "        # Calculate gradients\n",
        "        dw = (1 / num_samples) * np.dot(X.T, (model_predictions - y)) + alpha * (\n",
        "                l1_ratio * np.sign(weights) + (1 - l1_ratio) * weights)\n",
        "        db = (1 / num_samples) * np.sum(model_predictions - y)\n",
        "\n",
        "        # Update weights and bias\n",
        "        weights -= learning_rate * dw\n",
        "        bias -= learning_rate * db\n",
        "\n",
        "        # Print debugging information every 100 iterations\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Iteration {i}: Weights: {weights}, Bias: {bias}, dw: {dw}, db: {db}\")\n",
        "\n",
        "    return weights, bias\n",
        "\n",
        "def main():\n",
        "    # Load preprocessed dataset\n",
        "    DATASET_PATH = '/content/preprocessed_candidates.csv'  # Update with Colab path\n",
        "    df = pd.read_csv(DATASET_PATH)\n",
        "\n",
        "    # Separate features and target variable\n",
        "    target_column = 'target'\n",
        "    X = df.drop(columns=[target_column])\n",
        "    y = df[target_column].values\n",
        "\n",
        "    # Print shapes and types for debugging\n",
        "    print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
        "    print(f\"X data types:\\n{X.dtypes}\")\n",
        "\n",
        "    # Split data into training and testing sets\n",
        "    num_samples = X.shape[0]\n",
        "    train_size = int(0.8 * num_samples)\n",
        "\n",
        "    # Use NumPy for the split to avoid using sklearn\n",
        "    X_train, X_test = X.values[:train_size], X.values[train_size:]\n",
        "    y_train, y_test = y[:train_size], y[train_size:]\n",
        "\n",
        "    # Train the Elastic Net model\n",
        "    weights, bias = elastic_net(X_train, y_train)\n",
        "\n",
        "    # Save the weights and bias\n",
        "    np.save('/content/weights.npy', weights)\n",
        "    np.save('/content/bias.npy', bias)\n",
        "\n",
        "    # Print weights and bias\n",
        "    print(\"Weights:\", weights)\n",
        "    print(\"Bias:\", bias)\n",
        "\n",
        "# Run the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "kMR0_GZMsvWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Elasticnet"
      ],
      "metadata": {
        "id": "U4rQWQT7xMTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class ElasticNet:\n",
        "    def __init__(self, alpha=1.0, l1_ratio=0.5, lr=0.01, iterations=1000):\n",
        "        self.alpha = alpha            # Regularization strength (lambda)\n",
        "        self.l1_ratio = l1_ratio      # Mix between L1 and L2 regularization\n",
        "        self.lr = lr                  # Learning rate\n",
        "        self.iterations = iterations   # Number of iterations (epochs)\n",
        "\n",
        "    def _compute_cost(self, X, y, y_pred, theta):\n",
        "        m = len(y)\n",
        "        mse = (1 / (2 * m)) * np.sum((y_pred - y) ** 2)\n",
        "        l1_penalty = self.l1_ratio * np.sum(np.abs(theta))\n",
        "        l2_penalty = (1 - self.l1_ratio) * np.sum(theta ** 2)\n",
        "        return mse + self.alpha * (l1_penalty + l2_penalty)\n",
        "\n",
        "    def _compute_gradient(self, X, y, y_pred, theta):\n",
        "        m = len(y)\n",
        "        gradient = (1 / m) * X.T.dot(y_pred - y) + \\\n",
        "                   self.alpha * (self.l1_ratio * np.sign(theta) + (1 - self.l1_ratio) * theta)\n",
        "        return gradient\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        m, n = X.shape\n",
        "        self.theta = np.zeros(n)  # Initialize theta to zeros\n",
        "        y = y.astype(float)        # Ensure y is a float\n",
        "\n",
        "        # Gradient Descent Loop\n",
        "        for _ in range(self.iterations):\n",
        "            y_pred = X.dot(self.theta)\n",
        "            cost = self._compute_cost(X, y, y_pred, self.theta)\n",
        "            gradient = self._compute_gradient(X, y, y_pred, self.theta)\n",
        "            self.theta -= self.lr * gradient\n",
        "\n",
        "    def predict(self, X):\n",
        "        return X.dot(self.theta)\n",
        "\n",
        "def main():\n",
        "    # Load preprocessed dataset\n",
        "    DATASET_PATH = '/content/preprocessed_candidates.csv'  # Update with Colab path\n",
        "    df = pd.read_csv(DATASET_PATH)\n",
        "\n",
        "    # Separate features and target variable\n",
        "    target_column = 'target'\n",
        "    X = df.drop(columns=[target_column]).values\n",
        "    y = df[target_column].values\n",
        "\n",
        "    # Split data into training and testing sets\n",
        "    num_samples = X.shape[0]\n",
        "    train_size = int(0.8 * num_samples)\n",
        "    X_train, X_test = X[:train_size], X[train_size:]\n",
        "    y_train, y_test = y[:train_size], y[train_size:]\n",
        "\n",
        "    # Create and fit the Elastic Net model\n",
        "    model = ElasticNet(alpha=1.0, l1_ratio=0.5, lr=0.01, iterations=1000)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    predictions = model.predict(X_test)\n",
        "\n",
        "    # Print the predictions\n",
        "    print(\"Predictions on the test set:\", predictions)\n",
        "\n",
        "    # Save the model parameters\n",
        "    np.save('/content/elastic_net_theta.npy', model.theta)\n",
        "\n",
        "# Run the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "xbltrEjWxPD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate"
      ],
      "metadata": {
        "id": "FsBmYKGGxZRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def elastic_net(X, y, weights, bias, alpha=1.0, l1_ratio=0.5, num_iterations=1000, learning_rate=0.001):\n",
        "    \"\"\"Evaluate the Elastic Net model.\"\"\"\n",
        "    num_samples = X.shape[0]\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        # Calculate model predictions\n",
        "        model_predictions = np.dot(X, weights) + bias\n",
        "\n",
        "        # Calculate gradients\n",
        "        dw = (1 / num_samples) * np.dot(X.T, (model_predictions - y)) + alpha * (\n",
        "                    l1_ratio * np.sign(weights) + (1 - l1_ratio) * weights)\n",
        "        db = (1 / num_samples) * np.sum(model_predictions - y)\n",
        "\n",
        "        # Update weights and bias\n",
        "        weights -= learning_rate * dw\n",
        "        bias -= learning_rate * db\n",
        "\n",
        "    return model_predictions\n",
        "\n",
        "def main():\n",
        "    # Load preprocessed dataset\n",
        "    DATASET_PATH = '/content/preprocessed_candidates.csv'  # Update with Colab path\n",
        "    df = pd.read_csv(DATASET_PATH)\n",
        "\n",
        "    # Separate features and target variable\n",
        "    target_column = 'target'\n",
        "    X = df.drop(columns=[target_column]).values\n",
        "    y = df[target_column].values\n",
        "\n",
        "    # Load weights and bias\n",
        "    weights = np.load('/content/weights.npy')  # Update with Colab path\n",
        "    bias = np.load('/content/bias.npy')  # Update with Colab path\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = elastic_net(X, y, weights, bias)\n",
        "\n",
        "    # Calculate Mean Squared Error\n",
        "    mse = np.mean((y_pred - y) ** 2)\n",
        "    print(\"Mean Squared Error:\", mse)\n",
        "\n",
        "    # Save predictions\n",
        "    np.save('/content/y_pred.npy', y_pred)  # Update with Colab path\n",
        "\n",
        "# Run the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "VWtOSTJtxcja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and Evaluate"
      ],
      "metadata": {
        "id": "nR_rMIJXxojp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load preprocessed data\n",
        "data_path = \"/content/preprocessed_candidates.csv\"  # Update to Colab path\n",
        "data = pd.read_csv(data_path)\n",
        "\n",
        "# Prepare features (X) and target (y)\n",
        "X = data.drop(columns=['target']).values\n",
        "y = data['target'].values\n",
        "\n",
        "# Feature scaling (Standardization)\n",
        "mean = np.mean(X, axis=0)\n",
        "std = np.std(X, axis=0)\n",
        "X = (X - mean) / std\n",
        "\n",
        "# Define Elastic Net\n",
        "def elastic_net(X, y, learning_rate=0.0001, n_iterations=1000):\n",
        "    n_samples, n_features = X.shape\n",
        "    weights = np.zeros(n_features)\n",
        "    bias = 0.0\n",
        "\n",
        "    for i in range(n_iterations):\n",
        "        model_predictions = np.dot(X, weights) + bias\n",
        "\n",
        "        # Compute gradients\n",
        "        dw = (1 / n_samples) * np.dot(X.T, (model_predictions - y))\n",
        "        db = (1 / n_samples) * np.sum(model_predictions - y)\n",
        "\n",
        "        # Update weights and bias\n",
        "        weights -= learning_rate * dw\n",
        "        bias -= learning_rate * db\n",
        "\n",
        "        if i % 100 == 0:  # Print every 100 iterations\n",
        "            print(f\"Iteration {i}: Weights: {weights}, Bias: {bias}, dw: {dw}, db: {db}\")\n",
        "\n",
        "    return weights, bias\n",
        "\n",
        "# Train the model\n",
        "weights, bias = elastic_net(X, y)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = np.dot(X, weights) + bias\n",
        "\n",
        "# Calculate Mean Squared Error\n",
        "mse = np.mean((y_pred - y) ** 2)\n",
        "print(f\"Mean Squared Error: {mse}\")\n"
      ],
      "metadata": {
        "id": "pS6KvTl2xrmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization"
      ],
      "metadata": {
        "id": "E0jTy-z-yC9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def main():\n",
        "    # Load actual and predicted values\n",
        "    y_true = np.load('/content/y_true.npy')  # Update to Colab path\n",
        "    y_pred = np.load('/content/y_pred.npy')  # Update to Colab path\n",
        "\n",
        "    # Create a scatter plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(y_true, y_pred, alpha=0.7)\n",
        "    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--')  # line for perfect prediction\n",
        "    plt.xlabel('Actual Values')\n",
        "    plt.ylabel('Predicted Values')\n",
        "    plt.title('Actual vs Predicted Values')\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "# Run the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "QAgnbkdMyGnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def main():\n",
        "    # Load actual and predicted values\n",
        "    y_true = np.load('/content/y_true.npy')  # Update to Colab path\n",
        "    y_pred = np.load('/content/y_pred.npy')  # Update to Colab path\n",
        "\n",
        "    # Create a line plot for actual vs predicted\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(y_true, label='Actual', alpha=0.7)\n",
        "    plt.plot(y_pred, label='Predicted', alpha=0.7)\n",
        "    plt.xlabel('Sample Index')\n",
        "    plt.ylabel('Values')\n",
        "    plt.title('Actual vs Predicted Values Over Samples')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "# Run the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "-WOia1iyyNhB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}